{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64767cb7-9ab1-448a-b916-4c62480e6b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLOv7\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14ab19a9-8844-4a08-82c3-01376efa613c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change directory to download dataset into that folder\n",
    "# cd '/Users/huntersylvester/Desktop/UMMC/Research/Moradi/xmlfolder1/'\n",
    "!cd \"C:\\\\Users\\\\hrmor\\\\OneDrive - University of Mississippi Medical Center\\\\05_Pycharm\\\\OAI_pain\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0851410d-a148-4ae3-95b0-0af37107161d",
   "metadata": {},
   "source": [
    "Selecting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "689d1d14-f25b-4b46-87c5-b87373262ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries needed\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os \n",
    "import glob \n",
    "import pickle\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from os import listdir, getcwd\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "772a24bd-cf8c-4bf1-8533-400014c0b0e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Downloading Dataset Version Zip in kneeJoint-1 to voc: 100% [1248465 / 1248465] bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Dataset Version Zip to kneeJoint-1 in voc:: 100%|████████████████████████| 105/105 [00:00<00:00, 826.81it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'kneeJoint-1\\\\data.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m rf \u001b[38;5;241m=\u001b[39m Roboflow(api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJWYD2mWWR1pKYW9rHCSw\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m project \u001b[38;5;241m=\u001b[39m rf\u001b[38;5;241m.\u001b[39mworkspace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone-d23ce\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mproject(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkneejoint\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mproject\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvoc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\roboflow\\core\\version.py:205\u001b[0m, in \u001b[0;36mVersion.download\u001b[1;34m(self, model_format, location, overwrite)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__download_zip(link, location, model_format)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__extract_zip(location, model_format)\n\u001b[1;32m--> 205\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__reformat_yaml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Dataset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mversion, model_format, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(location))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\roboflow\\core\\version.py:593\u001b[0m, in \u001b[0;36mVersion.__reformat_yaml\u001b[1;34m(self, location, format)\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    591\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m content\n\u001b[1;32m--> 593\u001b[0m \u001b[43mamend_data_yaml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\roboflow\\util\\annotations.py:8\u001b[0m, in \u001b[0;36mamend_data_yaml\u001b[1;34m(path, callback)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mamend_data_yaml\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m, callback: Callable[[\u001b[38;5;28mdict\u001b[39m], \u001b[38;5;28mdict\u001b[39m]):\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m source:\n\u001b[0;32m      9\u001b[0m         content \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(source)\n\u001b[0;32m     10\u001b[0m     content \u001b[38;5;241m=\u001b[39m callback(content)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'kneeJoint-1\\\\data.yaml'"
     ]
    }
   ],
   "source": [
    "# !pip install roboflow\n",
    "# Download faux dataset to make sure this works\n",
    "# It is in same format as our actual images and annotations (XML format) and knee images\n",
    "# This will create new folder called kneeJoint-1 in directory\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"JWYD2mWWR1pKYW9rHCSw\")\n",
    "project = rf.workspace(\"none-d23ce\").project(\"kneejoint\")\n",
    "dataset = project.version(1).download(\"voc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd48beb-e964-470f-bea9-2d53e6c9f8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths so all will work accordingly\n",
    "data_path = './kneeJoint-1'\n",
    "data_path = Path(data_path)\n",
    "images_path = data_path / 'train'\n",
    "annotations_file_path = data_path / 'out.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d21980-ea26-4445-9f49-ab8c041d01cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory should be in folder before images and labels: '/Users/huntersylvester/Desktop/UMMC/Research/Moradi/xmlfolder/kneeJoint-1'\n",
    "# cd ../xmlfolder1/kneeJoint-1/\n",
    "!cd \".\\kneeJoint-1\"\n",
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91308c06-30d2-403c-bdae-7a42acfb947c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Converts XML info to YOLO txt files to create data frames in upcoming steps\n",
    "\n",
    "# If you have test or more classes can always include below\n",
    "dirs = ['train', 'valid']\n",
    "classes = ['KneeJoint']\n",
    "\n",
    "def getImagesInDir(dir_path):\n",
    "    image_list = []\n",
    "    for filename in glob.glob(dir_path + '/*.jpg'):\n",
    "        image_list.append(filename)\n",
    "\n",
    "    return image_list\n",
    "\n",
    "# This provides center x,y width and height\n",
    "# def convert(size, box):\n",
    "#     dw = 1./(size[0])\n",
    "#     dh = 1./(size[1])\n",
    "#     x = (box[0] + box[1])/2.0 - 1\n",
    "#     y = (box[2] + box[3])/2.0 - 1\n",
    "#     w = box[1] - box[0]\n",
    "#     h = box[3] - box[2]\n",
    "#     x = x*dw\n",
    "#     w = w*dw\n",
    "#     y = y*dh\n",
    "#     h = h*dh\n",
    "#     return (x,y,w,h)\n",
    "\n",
    "# This just gives xmin, ymin, etc. \n",
    "def convert(size, box):\n",
    "    dw = 1./(size[0])\n",
    "    dh = 1./(size[1])\n",
    "    xmin = box[0]\n",
    "    xmax = box[1]\n",
    "    ymin = box[2] \n",
    "    ymax = box[3]\n",
    "\n",
    "    return (xmin,xmax,ymin,ymax)\n",
    "    \n",
    "\n",
    "def convert_annotation(dir_path, output_path, image_path):\n",
    "    basename = os.path.basename(image_path)\n",
    "    basename_no_ext = os.path.splitext(basename)[0]\n",
    "\n",
    "    in_file = open(dir_path + '/' + basename_no_ext + '.xml')\n",
    "    out_file = open(output_path + basename_no_ext + '.txt', 'w')\n",
    "    tree = ET.parse(in_file)\n",
    "    root = tree.getroot()\n",
    "    size = root.find('size')\n",
    "    w = int(size.find('width').text)\n",
    "    h = int(size.find('height').text)\n",
    "\n",
    "    for obj in root.iter('object'):\n",
    "        difficult = obj.find('difficult').text\n",
    "        cls = obj.find('name').text\n",
    "        if cls not in classes or int(difficult)==1:\n",
    "            continue\n",
    "        cls_id = classes.index(cls)\n",
    "        xmlbox = obj.find('bndbox')\n",
    "        b = (float(xmlbox.find('xmin').text), float(xmlbox.find('xmax').text), float(xmlbox.find('ymin').text), float(xmlbox.find('ymax').text))\n",
    "        bb = convert((w,h), b)\n",
    "        out_file.write(str(cls_id) + \" \" + \" \".join([str(a) for a in bb]) + '\\n')\n",
    "        \n",
    "cwd = getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730b2dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cwd)\n",
    "cwd+=\"\\\\kneeJoint-1\\\\\"\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc9fbd4-a439-450c-8b14-20f80e132a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This feeds files into functions in previous cell \n",
    "for dir_path in dirs:\n",
    "    full_dir_path = cwd + '/' + dir_path\n",
    "    output_path = full_dir_path +'/yolo/'\n",
    "\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    image_paths = getImagesInDir(full_dir_path)\n",
    "    list_file = open(full_dir_path + '.txt', 'w')\n",
    "\n",
    "    for image_path in image_paths:\n",
    "        list_file.write(image_path + '\\n')\n",
    "        convert_annotation(full_dir_path, output_path, image_path)\n",
    "    list_file.close()\n",
    "\n",
    "    print(\"Finished processing: \" + dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a69f8a-1f39-4522-b127-0225d5f1afba",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# May get errors so use to delete ipynb.checkpoint \n",
    "# ls -l train/yolo/\n",
    "# rm -rf find -type d -name .ipynb_checkpoints\n",
    "# cd '/Users/huntersylvester/Desktop/UMMC/Research/Moradi/xmlfolder/kneeJoint-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ae38e0-91f8-4b24-8a1e-5725d19328b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates dataframe to save\n",
    "# Must change path to yolo folder now\n",
    "cwd = getcwd()\n",
    "# folder_path = cwd + '/train/yolo' \n",
    "folder_path = cwd + '\\\\kneeJoint-1\\\\train\\\\yolo' \n",
    "folder_path\n",
    "# folder_path = \"/Users/huntersylvester/Desktop/UMMC/Research/Moradi/xmlfolder1/kneeJoint-1/train/yolo\"\n",
    "list1 = []\n",
    "\n",
    "print(os.listdir(folder_path)[0])\n",
    "for fldr in os.listdir(folder_path): # 91c36b72-9117692L_png.rf.8c2578579a76e03acc7102a2f4066a52.txt\n",
    "    y = fldr.split('.txt')[0] # 91c36b72-9117692L_png.rf.8c2578579a76e03acc7102a2f4066a52\n",
    "    x = \"kneeJoint-1\\\\train\\\\yolo\\\\\" + fldr\n",
    "    with open(x) as f:\n",
    "        for line in f:\n",
    "            j = line.strip('\\n').split(' ') # ['0', '0.5109375', '0.51953125', '0.92578125', '0.28359375']\n",
    "            j.append(y)\n",
    "            list1.append(j)\n",
    "df = pd.DataFrame(list1, columns=['label', 'xmin', 'xmax', 'ymin', 'ymax', 'image'])\n",
    "\n",
    "# Either way below works\n",
    "df.label[df.label=='0'] = 'KneeJoint'\n",
    "# df.label[df.label=='0'] = df['label'].map({0:'KneeJoint'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd1f41e-9970-4323-83c6-d482bacbfc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2350c72c-84ab-44be-8f7e-5d13457d3503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to save file as a csv\n",
    "# set directory beforehand\n",
    "df.to_csv('kneeJoint-1\\\\out.csv')\n",
    "# df.to_csv('out.csv',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6511dbb7-3cad-40fd-9466-f64abe23d833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "def load_cars_df(annotations_file_path, images_path):\n",
    "    all_images = sorted(set([p.parts[-1] for p in images_path.iterdir()]))\n",
    "    image_id_to_image = {i: im for i, im in enumerate(all_images)}\n",
    "    image_to_image_id = {v: k for k, v, in image_id_to_image.items()}\n",
    "\n",
    "    annotations_df = pd.read_csv(annotations_file_path)\n",
    "    annotations_df.loc[:, \"class_name\"] = \"kneejoint\"\n",
    "    annotations_df.loc[:, \"has_annotation\"] = True\n",
    "\n",
    "    # add 10 empty images to the dataset\n",
    "#     empty_images = sorted(set(all_images) - set(annotations_df.ImageID.unique()))\n",
    "#     non_annotated_df = pd.DataFrame(list(empty_images)[:5], columns=[\"image\"])\n",
    "#     non_annotated_df.loc[:, \"has_annotation\"] = False\n",
    "#     non_annotated_df.loc[:, \"class_name\"] = \"background\"\n",
    "\n",
    "#     df = pd.concat((annotations_df, non_annotated_df))\n",
    "#     df = pd.concat((annotations_df, ))\n",
    "    df = annotations_df\n",
    "    \n",
    "    class_id_to_label = dict(\n",
    "        enumerate(df.query(\"has_annotation == True\").class_name.unique())\n",
    "    )\n",
    "    class_label_to_id = {v: k for k, v in class_id_to_label.items()}\n",
    "\n",
    "    df[\"image_id\"] = df.image.map(image_to_image_id)\n",
    "    df[\"class_id\"] = df.class_name.map(class_label_to_id)\n",
    "\n",
    "    file_names = tuple(df.image.unique())\n",
    "    random.seed(42)\n",
    "    validation_files = set(random.sample(file_names, int(len(df) * 0.1)))\n",
    "    train_df = df[~df.image.isin(validation_files)]\n",
    "    valid_df = df[df.image.isin(validation_files)]\n",
    "\n",
    "    lookups = {\n",
    "        \"image_id_to_image\": image_id_to_image,\n",
    "        \"image_to_image_id\": image_to_image_id,\n",
    "        \"class_id_to_label\": class_id_to_label,\n",
    "        \"class_label_to_id\": class_label_to_id,\n",
    "    }\n",
    "    return train_df, valid_df, lookups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d5f6dd-3c87-4597-b09b-51d5c8057b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can't get this to work\n",
    "# data_path = 'Desktop/UMMC/Research/Moradi/xmlfolder/kneeJoint-1'\n",
    "\n",
    "# annotations_file_path = .\\kneeJoint-1\\out.csv\n",
    "# images_path = .\\kneeJoint-1\\train\\\n",
    "train_df, valid_df, lookups = load_cars_df(annotations_file_path, images_path)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7069132-3796-4ce9-ac47-637fbe4008ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drops a column not used and transfers id to image_id\n",
    "train_df['image_id'] = train_df['Unnamed: 0']\n",
    "train_df.drop(columns=['Unnamed: 0', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5aa760-f000-4e6f-935a-57d2df321ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookups.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b75ed87-bdac-490d-9fb9-18dfb4e720eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookups['class_label_to_id'], lookups['class_id_to_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5543e02-1d8d-484e-9abf-2686eb5f8366",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Num. annotated images in training set: {len(train_df.query('has_annotation == True').image.unique())}\")\n",
    "print(f\"Num. Background images in training set: {len(train_df.query('has_annotation == False').image.unique())}\")\n",
    "print(f\"Total Num. images in training set: {len(train_df.image.unique())}\")\n",
    "print('------------')\n",
    "\n",
    "print(f\"Num. annotated images in validation set: {len(valid_df.query('has_annotation == True').image.unique())}\")\n",
    "print(f\"Num. Background images in validation set: {len(valid_df.query('has_annotation == False').image.unique())}\")\n",
    "print(f\"Total Num. images in validation set: {len(valid_df.image.unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6002c71-6352-4b0e-898b-d17ad297d3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarsDatasetAdaptor():\n",
    "    def __init__(\n",
    "        self,\n",
    "        images_dir_path,\n",
    "        annotations_dataframe,\n",
    "        transforms=None,\n",
    "    ):\n",
    "        self.images_dir_path = Path(images_dir_path)\n",
    "        self.annotations_df = annotations_dataframe\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.image_idx_to_image_id = {\n",
    "            idx: image_id\n",
    "            for idx, image_id in enumerate(self.annotations_df.image_id.unique())\n",
    "        }\n",
    "        self.image_id_to_image_idx = {\n",
    "            v: k for k, v, in self.image_idx_to_image_id.items()\n",
    "        }\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_idx_to_image_id)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_id = self.image_idx_to_image_id[index]\n",
    "        image_info = self.annotations_df[self.annotations_df.image_id == image_id]\n",
    "        file_name = image_info.image.values[0]\n",
    "        assert image_id == image_info.image_id.values[0]\n",
    "        file_name = file_name + '.jpg'\n",
    "#         image = Image.open(self.images_dir_path / file_name).convert(\"RGB\")\n",
    "        image = Image.open(self.images_dir_path / file_name).convert(\"L\")\n",
    "        image = np.array(image)\n",
    "\n",
    "        image_hw = image.shape[:2]\n",
    "\n",
    "        if image_info.has_annotation.any():\n",
    "            xyxy_bboxes = image_info[[\"xmin\", \"ymin\", \"xmax\", \"ymax\"]].values\n",
    "            class_ids = image_info[\"class_id\"].values\n",
    "        else:\n",
    "            xyxy_bboxes = np.array([])\n",
    "            class_ids = np.array([])\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            transformed = self.transforms(\n",
    "                image=image, bboxes=xyxy_bboxes, labels=class_ids\n",
    "            )\n",
    "            image = transformed[\"image\"]\n",
    "            xyxy_bboxes = np.array(transformed[\"bboxes\"])\n",
    "            class_ids = np.array(transformed[\"labels\"])\n",
    "\n",
    "        return image, xyxy_bboxes, class_ids, image_id, image_hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31bf57f-1ec8-4e5d-a38b-d4c903ff61fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = CarsDatasetAdaptor(images_path, train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e8fbd1-782d-453a-aa08-e685f5fde143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks to make sure same size as training file\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb87740-df06-45d3-9daf-283bed763922",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "\n",
    "\n",
    "def get_rectangle_params_from_coco_bbox(bbox):\n",
    "    x_min, y_min, width, height = bbox\n",
    "\n",
    "    bottom_left = (x_min, y_min)\n",
    "\n",
    "    return bottom_left, width, height\n",
    "\n",
    "\n",
    "def get_rectangle_params_from_yolo_bbox(bbox):\n",
    "    cx, cy, width, height = bbox\n",
    "\n",
    "    x_min = cx - width / 2\n",
    "    y_min = cy - height / 2\n",
    "\n",
    "    bottom_left = (x_min, y_min)\n",
    "\n",
    "    return bottom_left, width, height\n",
    "\n",
    "\n",
    "def get_rectangle_params_from_pascal_bbox(bbox):\n",
    "    xmin_top_left, ymin_top_left, xmax_bottom_right, ymax_bottom_right = bbox\n",
    "\n",
    "    bottom_left = (xmin_top_left, ymax_bottom_right)\n",
    "    width = xmax_bottom_right - xmin_top_left\n",
    "    height = ymin_top_left - ymax_bottom_right\n",
    "\n",
    "    return bottom_left, width, height\n",
    "\n",
    "\n",
    "def draw_bboxes(\n",
    "    plot_ax,\n",
    "    bboxes,\n",
    "    class_labels=None,\n",
    "    color_1=\"black\",\n",
    "    color_2=\"white\",\n",
    "    get_rectangle_corners_fn=get_rectangle_params_from_pascal_bbox,\n",
    "):\n",
    "    if class_labels is not None:\n",
    "        assert len(class_labels) == len(bboxes)\n",
    "    else:\n",
    "        class_labels = [None] * len(bboxes)\n",
    "\n",
    "    for bbox, label in zip(bboxes, class_labels):\n",
    "        bottom_left, width, height = get_rectangle_corners_fn(bbox)\n",
    "\n",
    "        rect_1 = patches.Rectangle(\n",
    "            bottom_left,\n",
    "            width,\n",
    "            height,\n",
    "            linewidth=4,\n",
    "            edgecolor=color_1,\n",
    "            fill=False,\n",
    "        )\n",
    "        rect_2 = patches.Rectangle(\n",
    "            bottom_left,\n",
    "            width,\n",
    "            height,\n",
    "            linewidth=2,\n",
    "            edgecolor=color_2,\n",
    "            fill=False,\n",
    "        )\n",
    "\n",
    "        # Add the patch to the Axes\n",
    "        plot_ax.add_patch(rect_1)\n",
    "        plot_ax.add_patch(rect_2)\n",
    "\n",
    "        if label is not None:\n",
    "            rx, ry = rect_1.get_xy()\n",
    "            plot_ax.annotate(label, (rx, ry + height), color=color_2, fontsize=20)\n",
    "\n",
    "\n",
    "draw_bboxes_coco = partial(\n",
    "    draw_bboxes, get_rectangle_corners_fn=get_rectangle_params_from_coco_bbox\n",
    ")\n",
    "\n",
    "draw_bboxes_yolo = partial(\n",
    "    draw_bboxes, get_rectangle_corners_fn=get_rectangle_params_from_yolo_bbox\n",
    ")\n",
    "\n",
    "draw_xyxy_bboxes = partial(\n",
    "    draw_bboxes, get_rectangle_corners_fn=get_rectangle_params_from_pascal_bbox\n",
    ")\n",
    "\n",
    "draw_functions = {\n",
    "    \"coco\": draw_bboxes_coco,\n",
    "    \"cxcywh\": draw_bboxes_yolo,\n",
    "    \"xyxy\": draw_xyxy_bboxes,\n",
    "}\n",
    "\n",
    "\n",
    "def annotate_image(\n",
    "    image, bboxes=None, class_labels=None, bbox_format=\"xyxy\", close_fig=True\n",
    "):\n",
    "    draw_bboxes_fn = draw_functions[bbox_format]\n",
    "\n",
    "    fig, ax = plt.subplots(1, figsize=(10, 10))\n",
    "    ax.imshow(image)\n",
    "\n",
    "    if bboxes:\n",
    "        draw_bboxes_fn(ax, bboxes=bboxes, class_labels=class_labels)\n",
    "\n",
    "    if close_fig:\n",
    "        plt.close(fig)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def show_image(image, bboxes=None, class_labels=None, bbox_format=\"xyxy\"):\n",
    "    fig = annotate_image(\n",
    "        image,\n",
    "        bboxes=bboxes,\n",
    "        class_labels=class_labels,\n",
    "        bbox_format=bbox_format,\n",
    "        close_fig=False,\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd27de81-aae0-45aa-9f06-4f1d8b7d50a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "idx = 4\n",
    "image, xyxy_bboxes, class_ids, image_idx, image_size = ds[idx]\n",
    "show_image(image, xyxy_bboxes.tolist(), [lookups['class_id_to_label'][int(c)] for c in class_ids])\n",
    "print(f'Image id: {image_idx}')\n",
    "print(f'Image size: {image_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590b80e7-4304-4048-ade1-679e4e84f76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 10\n",
    "image, xyxy_bboxes, class_ids, image_idx, image_size = ds[idx]\n",
    "show_image(image, xyxy_bboxes.tolist(), [lookups['class_id_to_label'][int(c)] for c in class_ids])\n",
    "print(f'Image id: {image_idx}')\n",
    "print(f'Image size: {image_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294540e5-0183-4165-891a-16a82b3dbb6e",
   "metadata": {},
   "source": [
    "# This is where I am having issues.  Albumentations is not working for me or importing properly.  Futhermore, I am having issues when I try to conver this to a tensor through Pytorch it will say not same index.  The errors are below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a240ef-c3f0-4e55-85ad-4577cd420c42",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # !pip install albumentations\n",
    "# import albumentations as A\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torchvision\n",
    "# from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "# def yolov7_collate_fn(batch):\n",
    "#     images, labels, indices, image_sizes = zip(*batch)\n",
    "#     for i, l in enumerate(labels):\n",
    "#         l[:, 0] = i  # add target image index for build_targets() in loss fn\n",
    "#     return (\n",
    "#         torch.stack(images, 0),\n",
    "#         torch.cat(labels, 0),\n",
    "#         torch.stack(indices, 0),\n",
    "#         torch.stack(image_sizes, 0),\n",
    "#     )\n",
    "\n",
    "\n",
    "# def create_base_transforms(target_image_size):\n",
    "#     return A.Compose(\n",
    "#         [\n",
    "#             A.LongestMaxSize(target_image_size),\n",
    "#         ],\n",
    "#         bbox_params=A.BboxParams(format=\"pascal_voc\", label_fields=[\"labels\"]),\n",
    "#     )\n",
    "\n",
    "\n",
    "# def create_yolov7_transforms(\n",
    "#     image_size=(640, 640),\n",
    "#     training=False,\n",
    "#     training_transforms=(A.HorizontalFlip(p=0.5),),\n",
    "# ):\n",
    "#     transforms = [\n",
    "#         A.LongestMaxSize(max(image_size)),\n",
    "#         A.PadIfNeeded(\n",
    "#             image_size[0],\n",
    "#             image_size[1],\n",
    "#             border_mode=0,\n",
    "#             value=(114, 114, 114),\n",
    "#         ),\n",
    "#     ]\n",
    "\n",
    "#     if training:\n",
    "#         transforms.extend(training_transforms)\n",
    "\n",
    "#     return A.Compose(\n",
    "#         transforms,\n",
    "#         bbox_params=A.BboxParams(format=\"pascal_voc\", label_fields=[\"labels\"]),\n",
    "#     )\n",
    "\n",
    "\n",
    "# def convert_xyxy_to_cxcywh(bboxes):\n",
    "#     bboxes = bboxes.copy()\n",
    "#     bboxes[:, 2] = bboxes[:, 2] - bboxes[:, 0]\n",
    "#     bboxes[:, 3] = bboxes[:, 3] - bboxes[:, 1]\n",
    "#     bboxes[:, 0] = bboxes[:, 0] + bboxes[:, 2] * 0.5\n",
    "#     bboxes[:, 1] = bboxes[:, 1] + bboxes[:, 3] * 0.5\n",
    "#     return bboxes\n",
    "\n",
    "\n",
    "# class Yolov7Dataset(Dataset):\n",
    "#     \"\"\"\n",
    "#     A dataset which takes an object detection dataset returning (image, boxes, classes, image_id, image_hw)\n",
    "#     and applies the necessary preprocessing steps as required by Yolov7 models.\n",
    "#     By default, this class expects the image, boxes (N, 4) and classes (N,) to be numpy arrays,\n",
    "#     with the boxes in (x1,y1,x2,y2) format, but this behaviour can be modified by\n",
    "#     overriding the `load_from_dataset` method.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, dataset, transforms=None):\n",
    "#         self.ds = dataset\n",
    "#         self.transforms = transforms\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.ds)\n",
    "\n",
    "#     def load_from_dataset(self, index):\n",
    "#         image, boxes, classes, image_id, shape = self.ds[index]\n",
    "#         return image, boxes, classes, image_id, shape\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         image, boxes, classes, image_id, original_image_size = self.load_from_dataset(\n",
    "#             index\n",
    "#         )\n",
    "\n",
    "#         if self.transforms is not None:\n",
    "#             transformed = self.transforms(image=image, bboxes=boxes, labels=classes)\n",
    "#             image = transformed[\"image\"]\n",
    "#             boxes = np.array(transformed[\"bboxes\"])\n",
    "#             classes = np.array(transformed[\"labels\"])\n",
    "\n",
    "#         image = image / 255  # 0 - 1 range\n",
    "\n",
    "#         if len(boxes) != 0:\n",
    "#             # filter boxes with 0 area in any dimension\n",
    "#             valid_boxes = (boxes[:, 2] > boxes[:, 0]) & (boxes[:, 3] > boxes[:, 1])\n",
    "#             boxes = boxes[valid_boxes]\n",
    "#             classes = classes[valid_boxes]\n",
    "\n",
    "#             boxes = torchvision.ops.box_convert(\n",
    "#                 torch.as_tensor(boxes, dtype=torch.float32), \"xyxy\", \"cxcywh\"\n",
    "#             )\n",
    "#             boxes[:, [1, 3]] /= image.shape[0]  # normalized height 0-1\n",
    "#             boxes[:, [0, 2]] /= image.shape[1]  # normalized width 0-1\n",
    "#             classes = np.expand_dims(classes, 1)\n",
    "\n",
    "#             labels_out = torch.hstack(\n",
    "#                 (\n",
    "#                     torch.zeros((len(boxes), 1)),\n",
    "#                     torch.as_tensor(classes, dtype=torch.float32),\n",
    "#                     boxes,\n",
    "#                 )\n",
    "#             )\n",
    "#         else:\n",
    "#             labels_out = torch.zeros((0, 6))\n",
    "\n",
    "#         try:\n",
    "#             if len(image_id) > 0:\n",
    "#                 image_id_tensor = torch.as_tensor([])\n",
    "\n",
    "#         except TypeError:\n",
    "#             image_id_tensor = torch.as_tensor(image_id)\n",
    "\n",
    "#         return (\n",
    "#             torch.as_tensor(image.transpose(2, 0, 1), dtype=torch.float32),\n",
    "#             labels_out,\n",
    "#             image_id_tensor,\n",
    "#             torch.as_tensor(original_image_size),\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2276ec6d-cc43-43c1-ad7c-d3b2147a321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the same code as above, but now I have removed any albumentations\n",
    "import numpy as np\n",
    "import torch\n",
    "# !pip install torchvision\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def yolov7_collate_fn(batch):\n",
    "    images, labels, indices, image_sizes = zip(*batch)\n",
    "    for i, l in enumerate(labels):\n",
    "        l[:, 0] = i  # add target image index for build_targets() in loss fn\n",
    "    return (\n",
    "        torch.stack(images, 0),\n",
    "        torch.cat(labels, 0),\n",
    "        torch.stack(indices, 0),\n",
    "        torch.stack(image_sizes, 0),\n",
    "    )\n",
    "\n",
    "def convert_xyxy_to_cxcywh(bboxes):\n",
    "    bboxes = bboxes.copy()\n",
    "    bboxes[:, 2] = bboxes[:, 2] - bboxes[:, 0]\n",
    "    bboxes[:, 3] = bboxes[:, 3] - bboxes[:, 1]\n",
    "    bboxes[:, 0] = bboxes[:, 0] + bboxes[:, 2] * 0.5\n",
    "    bboxes[:, 1] = bboxes[:, 1] + bboxes[:, 3] * 0.5\n",
    "    return bboxes\n",
    "\n",
    "class Yolov7Dataset():\n",
    "    \"\"\"\n",
    "    A dataset which takes an object detection dataset returning (image, boxes, classes, image_id, image_hw)\n",
    "    and applies the necessary preprocessing steps as required by Yolov7 models.\n",
    "    By default, this class expects the image, boxes (N, 4) and classes (N,) to be numpy arrays,\n",
    "    with the boxes in (x1,y1,x2,y2) format, but this behaviour can be modified by\n",
    "    overriding the `load_from_dataset` method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, transforms=None):\n",
    "        self.ds = dataset\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def load_from_dataset(self, index):\n",
    "        image, boxes, classes, image_id, shape = self.ds[index]\n",
    "        return image, boxes, classes, image_id, shape\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, boxes, classes, image_id, original_image_size = self.load_from_dataset(\n",
    "            index\n",
    "        )\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            transformed = self.transforms(image=image, bboxes=boxes, labels=classes)\n",
    "            image = transformed[\"image\"]\n",
    "            boxes = np.array(transformed[\"bboxes\"])\n",
    "            classes = np.array(transformed[\"labels\"])\n",
    "\n",
    "        image = image / 255  # 0 - 1 range\n",
    "\n",
    "        if len(boxes) != 0:\n",
    "            # filter boxes with 0 area in any dimension\n",
    "            valid_boxes = (boxes[:, 2] > boxes[:, 0]) & (boxes[:, 3] > boxes[:, 1])\n",
    "            boxes = boxes[valid_boxes]\n",
    "            classes = classes[valid_boxes]\n",
    "\n",
    "            boxes = torchvision.ops.box_convert(\n",
    "                torch.as_tensor(boxes, dtype=torch.float32), \"xyxy\", \"cxcywh\"\n",
    "            )\n",
    "            boxes[:, [1, 3]] /= image.shape[0]  # normalized height 0-1\n",
    "            boxes[:, [0, 2]] /= image.shape[1]  # normalized width 0-1\n",
    "            classes = np.expand_dims(classes, 1)\n",
    "\n",
    "            labels_out = torch.hstack(\n",
    "                (\n",
    "                    torch.zeros((len(boxes), 1)),\n",
    "                    torch.as_tensor(classes, dtype=torch.float32),\n",
    "                    boxes,\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            labels_out = torch.zeros((0, 6))\n",
    "\n",
    "        try:\n",
    "            if len(image_id) > 0:\n",
    "                image_id_tensor = torch.as_tensor([])\n",
    "\n",
    "        except TypeError:\n",
    "            image_id_tensor = torch.as_tensor(image_id)\n",
    "\n",
    "        return (\n",
    "            torch.as_tensor(image.transpose(2, 0, 1), dtype=torch.float32),\n",
    "            labels_out,\n",
    "            image_id_tensor,\n",
    "            torch.as_tensor(original_image_size),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b061cac6-f468-4972-bf41-7c120d16dbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_ds = Yolov7Dataset(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e20b5b-1d13-4032-b38b-3d259ca60d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 4\n",
    "image_tensor, labels, image_id, image_size = yolo_ds[idx]\n",
    "\n",
    "print(f'Image: {image_tensor.shape}')\n",
    "print(f'Labels: {labels}')\n",
    "\n",
    "# denormalize boxes\n",
    "boxes = labels[:, 2:]\n",
    "boxes[:, [0, 2]] *= image_size[1]\n",
    "boxes[:, [1, 3]] *= image_size[0]\n",
    "\n",
    "show_image(image_tensor.permute( 1, 2, 0), boxes.tolist(), [lookups['class_id_to_label'][int(c)] for c in labels[:, 1]], 'cxcywh')\n",
    "print(f'Image id: {image_id}')\n",
    "print(f'Image size: {image_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eb8ca7-028f-4742-a420-fc39199dcf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
