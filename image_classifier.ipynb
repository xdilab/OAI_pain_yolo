{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "892c86c7-1222-48f7-a521-723a8b4031f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/huntersylvester/Desktop/UMMC/Research/Moradi'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "92d3aa7d-3d5e-48b8-a450-c8136be745f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "number of layers: 10\n",
      "number of layers: 10\n",
      "number of layers: 10\n",
      "number of layers: 10\n",
      "number of layers: 10\n",
      "number of layers: 10\n",
      "number of layers: 10\n",
      "8 11 384\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "[1, 0, 0, 0, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "import fnmatch\n",
    "import glob\n",
    "import os\n",
    "from natsort import natsorted\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Empty list to store numpy arrays\n",
    "image_list = []\n",
    "y = []\n",
    "\n",
    "# lets us know number of images in that file\n",
    "dirpath = \"/Users/huntersylvester/Desktop/UMMC/Research/Moradi/sample_knee_mri_localization/9009067_10287112_R_Progressed\"\n",
    "num_layers = len(fnmatch.filter(os.listdir(dirpath), '*.jpg'))\n",
    "print(num_layers)\n",
    "\n",
    "\n",
    "directory = \"/Users/huntersylvester/Desktop/UMMC/Research/Moradi/knee_mri_localization\"\n",
    "for filename in glob.iglob(f\"{directory}/*\"):\n",
    "    i = 0\n",
    "    os.chdir(filename)\n",
    "    x = filename.find('_NotProgressed')  # Will give -1 if phrase not in string\n",
    "    if x > 0:\n",
    "        y.append(0)  # non progressed 0\n",
    "    else:\n",
    "        y.append(1)  # progressed 1\n",
    "    image_patient = []\n",
    "    for file in natsorted(glob.glob(\"*.jpg\")):\n",
    "        i += 1\n",
    "        im = Image.open(file)\n",
    "        image_patient.append(np.array(im))\n",
    "        if i != num_layers:\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"number of layers: {i}\")\n",
    "    image_list.append(np.array(image_patient))\n",
    "\n",
    "image_list = np.array(image_list, dtype=object)\n",
    "# image_list = np.array(image_list, dtype=object)\n",
    "\n",
    "print(len(image_list), len(image_list[0]), len(image_list[0][0]))\n",
    "print(type(image_list), type(image_list[0]), type(image_list[0][0]))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ecee518-9e48-41db-b3d0-52cfd75b1858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fb1624ef-d553-421a-8202-ae4ce72795fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in train and validation are 4 and 4.\n"
     ]
    }
   ],
   "source": [
    "len(image_list[1]) # We get 11 layers for one patient same if we do len(image_list[2])\n",
    "image_list[1][1] # This is each image in a numpy for patient 1\n",
    "y[:3] # These are the labels for the first three patients so we should see 1 0 0 \n",
    "\n",
    "# For now split data into 50-50 for training and validation\n",
    "x_train = image_list[:4]\n",
    "y_train = y[:4]\n",
    "x_val   = image_list[4:]\n",
    "y_val   = y[4:]\n",
    "\n",
    "print(\n",
    "    \"Number of samples in train and validation are %d and %d.\"\n",
    "    % (x_train.shape[0], x_val.shape[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "afd16636-9be2-4305-b35e-cdfb68844315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We no longer are rotating \n",
    "def train_preprocessing(volume, label):\n",
    "    \"\"\"Process training data by rotating and adding a channel.\"\"\"\n",
    "    # Rotate volume\n",
    "#   volume = rotate(volume)\n",
    "    volume = tf.expand_dims(volume, axis=3)\n",
    "    return volume, label\n",
    "\n",
    "\n",
    "def validation_preprocessing(volume, label):\n",
    "    \"\"\"Process validation data by only adding a channel.\"\"\"\n",
    "    volume = tf.expand_dims(volume, axis=3)\n",
    "    return volume, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a79f1e25-801f-4ca2-84d8-f56b3ef1f0ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't convert non-rectangular Python sequence to Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-c4b14e75ee79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m############# This currently works as of now as a fix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mX_val\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# train_loader = tf.data.Dataset.from_tensor_slices((list(float(x_train)), y_train))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Can't convert non-rectangular Python sequence to Tensor."
     ]
    }
   ],
   "source": [
    "# Define data loaders.\n",
    "\n",
    "# We have to do this or else the third line will not work\n",
    "############# This currently works as of now as a fix\n",
    "X_train = tf.convert_to_tensor(list(x_train), dtype=tf.float32)\n",
    "X_val   = tf.convert_to_tensor(list(x_val), dtype=tf.float32)\n",
    "train_loader = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "# train_loader = tf.data.Dataset.from_tensor_slices((list(float(x_train)), y_train))\n",
    "validation_loader = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "\n",
    "batch_size = 2\n",
    "# Augment the on the fly during training.\n",
    "train_dataset = (\n",
    "    train_loader.shuffle(len(x_train))\n",
    "    .map(train_preprocessing)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(2)\n",
    ")\n",
    "# Only rescale.\n",
    "validation_dataset = (\n",
    "    validation_loader.shuffle(len(x_val))\n",
    "    .map(validation_preprocessing)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f180c072-b4ad-4d38-b7a9-5727d92f67c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the CT scan is: (11, 384, 384, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fbbfb2cccd0>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "####### As we can see it is labeling y axis as 11 and x axis as 384.  We need 384 x 384 for xy axis.  This is because we have skipped the process scan phase in the code\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = train_dataset.take(1)\n",
    "images, labels = list(data)[0]\n",
    "images = images.numpy()\n",
    "image = images[0]\n",
    "print(\"Dimension of the CT scan is:\", image.shape)\n",
    "plt.imshow(np.squeeze(image[:, :, 30]), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c289b0-31ae-498c-8ebb-aa2cf44d1abc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
